# FIGA
This repository is the official implementation of ICLR 2024 paper: **[Beyond Imitation: Leveraging Fine-grained Quality Signals for Alignment](https://arxiv.org/pdf/2311.04072.pdf)**. 


## Quick Start
Considering that a modified version of transformers will be installed, it is recommended to create a new conda environment:
```bash
conda create -n FIGA python=3.8
conda activate FIGA
conda install pytorch==2.1.0 torchvision==0.16.0 torchaudio==2.1.0 pytorch-cuda=12.1 -c pytorch -c nvidia
```
You should clone the FIGA repository and follow its instructions.
```bash
git clone https://github.com/RUCAIBox/FIGA.git && cd FIGA
pip install -r requirements.txt
```

After this, you need to replace the `trainer_utils.py` and `modeling_llama.py` files in the transformers library with the corresponding files from this repository. This is necessary for fine-tuning using the FIGA method.

## SPA Dataset

You can download SPA dataset in: https://huggingface.co/datasets/RUCAIBox/SPA. You should download SPA dataset and place it in the `data` folder.

For our publicly available SPA dataset, the `raw` field is the ground truth response, the `output` field contains results generated by the alpaca-7b model, and the `revised_output` field contains results modified by using a more powerful model (ChatGPT-3.5). For a detailed description of the construction process of the SPA dataset, please refer to our paper.


## Instruction tuning and Inference
After setting up the environment, you can train models on your own dataset using FIGA method from scratch.


### Tuning
To train your own model on another dataset from scratch, you can go as below.

```bash
bash FIGA/bash/run_7b.sh > output.log 2>&1
```
### Inference

For inference, you can use the transformers interface just like with any other model to perform inference:
```python
from transformers import AutoModelForCausalLM, AutoTokenizer

prompt = "Complete the following sentence.\n\nThe mine stopped producing the iron and started mining the gold instead because the"
tokenizer = AutoTokenizer.from_pretrained("/path/to/FIGA-model")
inputs = tokenizer(prompt, return_tensors="pt").input_ids
model = AutoModelForCausalLM.from_pretrained("/path/to/FIGA-model")
outputs = model.generate(inputs, max_new_tokens=100, do_sample=True, top_k=50, top_p=0.95)
```
## Acknowledgment

Please cite the following paper if you find our code or data helpful.

@article{guo2023beyond,
  title={Beyond imitation: Leveraging fine-grained quality signals for alignment},
  author={Guo, Geyang and Zhao, Ranchi and Tang, Tianyi and Zhao, Wayne Xin and Wen, Ji-Rong},
  journal={arXiv preprint arXiv:2311.04072},
  year={2023}
}

